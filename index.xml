<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>蜗牛保持乐观才会爬的更远 on 付辉</title>
    <link>/</link>
    <description>Recent content in 蜗牛保持乐观才会爬的更远 on 付辉</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 01 Jul 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>MySQL事务</title>
      <link>/blog/2018/07-01-mysql%E4%BA%8B%E5%8A%A1/</link>
      <pubDate>Sun, 01 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/07-01-mysql%E4%BA%8B%E5%8A%A1/</guid>
      <description>关于MySQL事务的诡异问题，至今没有调查出原因。但却也是一个契机，带我重新回忆之前的遇到的事务问题。
诡异的问题 系统中存在A和B两个表。B表中有两个关键字段：一个是唯一索引transaction_id，还有一个是标识处理状态的status。当status=0表示记录未被处理，status=1表示记录处理过了，不需要再处理了。
如果B中记录未处理，则在A表中插入一条权益记录，同时更新status=1，后续就不能再给用户加权益了。
代码做了如下处理：
//简化的代码 func sessPart() { //开启事务 session := engine.NewSession() sess.Begin() defer session.Close() defer sess.Rollback() //插入价钱100分的权益交付记录 exchange := models.Exchange{Money: 100, Uid: 1} _, err := sess.Insert(exchange) if err != nil { sess.Rollback() return } //更新status为1 //并且使用乐观锁，防止因没有匹配到数据，直接返回成功 testModel := Test{ Status: 1, } affectRows, err := sess.Where(&amp;quot;transaction_id = ? AND status = 0&amp;quot;, 1). Cols(&amp;quot;status&amp;quot;).Update(&amp;amp;testModel) if err != nil || affectRows == 0 { sess.Rollback() return } sess.</description>
    </item>
    
    <item>
      <title>timewait状态解读</title>
      <link>/blog/2018/06-15-timewait%E7%8A%B6%E6%80%81%E8%A7%A3%E8%AF%BB/</link>
      <pubDate>Fri, 15 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/06-15-timewait%E7%8A%B6%E6%80%81%E8%A7%A3%E8%AF%BB/</guid>
      <description>突然想梳理一下time_wait,毕竟自己遇到它好多次了。
time_wait status time_wait作为HTTP连接关闭的一个正常状态。当系统time_wait过多，超过操作系统设定的文件套接字上限时，就会导致整个服务不可用。
唯一确定连接的4个组成部分，它们是客户端及服务端的IP和PORT。一般来说，处于time_wait状态的port在2mls内是无法被重复使用的。所以瞬间的wait_wait过多，直接导致整个系统无法服务。
关闭连接包含4次握手，TCP是全双工的，有一端需要主动提出关闭。相应的，对端来被动来关闭。对于我们常见的CS模式，主动和被动的角色是没有明确界限的。
client[FIN_WAIT_1] ---(FIN M)---&amp;gt; server[CLOSE_WAIT] client[FIN_WAIT_2] &amp;lt;---(ack M+1)--- server client[TIME_WAIT] &amp;lt;---(FIN N)--- server[LAST_ACK] client ---(ack N+1)---&amp;gt; server[CLOSED]  active close端的系统中才会出现time_wait的状态。拿请求https://google.com来举例，客户端在创建连接时，其实并不关心连接的端口号，它是系统随机创建的。google服务存在一个443端口,一直处于listen状态。当客户端断开连接时，客户端系统其实就会出现time_wait。当服务端主动断开连接时，客户端会出现close_wait状态。
2MLS time_wait也被称为2MLS wait。全名maximum segment lifetime, 表示一个数据块在被丢弃之前，在网络中能存在的最长时间。TCP的数据包是作为IP数据传输的，而IP数据包是否有效受限于设置的TTL，所以该MSL存在上限。
在2MLS内，该连接不会处理那些迟到的请求，占用的端口号也无法被系统的其他程序使用。2MLS还可以保证，当服务端没有收到ack时，客户端重新发送一次ack。
可以通过tcp_tw_reuse来重用time_wait状态的端口号。
shell查询time_wait连接 查看连接的状态，主要有两个命令netstat和ss。netstat有的ss都有，而且运行非常快。
netstat -n | awk &#39;/^tcp/ {++S[$NF]} END {for(a in S) {print a, S[a]} }&#39;  匹配tcp连接，声明了数组S，$NF用于获取最后一列的数据，也就是tcp status，最后通过for语句输出。
ss -o state time-wait &#39;( sport = :http )&#39; #timewait是中划线  通过ss还可以方便的过滤出源端口是80的，状态是time_wait的连接
总结 在开发中，可以适当考虑使用长连接。而且，现在基本所有的库都自带连接池功能。</description>
    </item>
    
    <item>
      <title>订单系统初识</title>
      <link>/blog/2018/06-08-%E8%AE%A2%E5%8D%95%E7%B3%BB%E7%BB%9F%E5%88%9D%E8%AF%86/</link>
      <pubDate>Fri, 08 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/06-08-%E8%AE%A2%E5%8D%95%E7%B3%BB%E7%BB%9F%E5%88%9D%E8%AF%86/</guid>
      <description>计划将订单系统做一下梳理，包括之前写的IAP支付。其中一些细节，自己描述的也不是特别满意。后续慢慢的完善。
整个订单系统，概括的讲，其实就是创建订单、支付订单、交付权益这三个过程。但里面涉及的东西，可能必我们想象的要多很多。
订单号 在调用支付时，一般都会要求创建订单号。该订单号是标识本地交易的凭证，必须做到系统唯一。
创建订单号也有很多学问。最差也得做到：首先，不重复；其次，别人不能通过订单号，推测出任何有意义的信息；再次，订单号长度一定要做好控制。
订单再支付 描述一个场景：用户下单之后，并没有立即支付，而是过了一段时间，重新支付之前下的订单。这种情况，如果涉及到扣减库存，还会设置订单的过期时间。
重新支付的时候可能会有点问题：很多支付接口，不允许使用同一个订单号支付两次，即使上一次没有支付成功。换句话说，当你想重新支付时，系统需要生成一个新的订单号。
这样会导致：用户仅仅成功购买了一个商品，后台却生成了数笔订单。后期的统计变得麻烦了不少。
我们的做法便是：在确认支付时，生成两个订单，一个父订单，一个子订单。父订单用来标识用户的购买行为，子订单用户跟第三方支付。
订单统一管理 对于业务简单的部门来说，订单号自己创建，自己管理完全足够了。
但公司想汇总各个部门的订单数据时会变得异常麻烦。所以需要统一的订单创建平台。该平台负责订单的创建、以及后续订单的状态管理。
整个系统也开始变得复杂起来了。订单创建、订单状态更新都需要通知“订单系统”。数据也开始出现不一致。
交付 在用户下单、支付成功之后，将购买的商品或权益给到用户。
实物商品 将用户的购买行为添加到他的购买记录等，让快递员将商品送到用户手中等。
虚拟商品 用户在王者荣耀上买了一套皮肤，这就属于购买虚拟商品。最终腾讯只需要给用户的数据库写一条权益记录就好，不存在发货的过程。
签收 拆单 用户可能一次性购买多个商品，但系统只生成一个订单。对于实物商品，会涉及的拆单流程。比如用户买了肥皂和书，后台系统需要将肥皂交给卖家A来发货，将书交给卖家B来发。同样的商品，可能还会根据卖家和买家的距离来拆单。
签收 那么当更新订单签收状态时，就有疑问了。一个订单，它可能对应多个商品，那么只有商品全部被签收成功，才应该修改为已签收状态。但这种全部成功或者全部失败的状态，本身就很难保证。
所以签收应该针对具体商品。商城系统中，有两个常用的概念：SPU和SKU。那小米手机来举例，小米Note可以当作是一个SPU,而具体的红色-32G等能具体到一个实际的个体的就是SKU。
最终，签收状态应该是订单号+SKU_ID来决定的。
表结构设计 在用户下单、支付的过程中，跟订单内部商品的SKU关系不大，而且为了保证订单的幂等性,将订单设置为唯一索引是必须的。
在签收的过程中，无法做到针对订单来签收，而应该对订单下的SKU做签收。所以签收表应该独立出来。</description>
    </item>
    
    <item>
      <title>shell操作文本实例</title>
      <link>/blog/2018/05-15-shell%E6%93%8D%E4%BD%9C%E6%96%87%E6%9C%AC%E5%AE%9E%E4%BE%8B/</link>
      <pubDate>Tue, 15 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/05-15-shell%E6%93%8D%E4%BD%9C%E6%96%87%E6%9C%AC%E5%AE%9E%E4%BE%8B/</guid>
      <description>从掌握awk的基本指令，到在工作中熟练使用，中间还有一段路要走！通过总结一些工作中需要的案例，来加深理解。
打印符合条件的前一行记录 这个案例很抽象，可能非常难遇到，除非自己给自己挖坑。
业务代码中处理每条数据，都会顺序输出两条日志。第一条表示要处理的数据内容，第二条表示处理的结果。现在想滤出来所有处理成功的记录。比如日志文件如下：
uid=1 HAPPY uid=2 SAD uid=3 SAD  处理的AWK脚本如下：
#! /bin/bash # testPage表示日志文件 pieces=$(awk &#39;/HAPPY/{line=NR-1;print line}&#39; testPage | xargs) for piece in $pieces do echo $(awk NR==$piece testPage) done  脚本总结：
 首先通过正则表达式过滤，获取执行成功的数据行号。然后传递给xargs，转换为空格分隔的字符串。 遍历每个行号。值得注意，shell中对空格分隔的字符串可以直接使用for...in 通过awk条件判断，打印该行内容  求解两个文件的差集 使用comm实现 存在A和B两个系统，理论上A系统中的数据都应该存在于B系统中。但当核对数据时，发现两者数据不一致。如何有效的找出数据的差集。
具体到真实环境，通过MySQL，导出了满足条件的A、B系统数据的id，文件格式是csv。但当我执行如下列命令，获取仅仅在文件a中存在的记录时，发现数据完全不正确。
comm -2 -3 a.txt b.txt  调查发现，需要将两个文本先排好序，才能正常返回。
sort a.txt &amp;gt; a-sort.txt sort b.txt &amp;gt; b-sort.txt comm -2 -3 a-sort.txt b-sort.txt  使用uniq及sort实现 如下这种方式，仅仅可以找出不匹配的记录。无法区分数据是仅仅存在A系统，还是仅仅存在B系统。只能获取数据的交集和差集两部分。
原理很简单，将两个系统的文件合并到一个文件，然后排序。最终交集的数据，应该有2条记录。差集的记录，只有1条。
cat a.txt b.</description>
    </item>
    
    <item>
      <title>代码重构</title>
      <link>/blog/2018/05-12-%E4%BB%A3%E7%A0%81%E9%87%8D%E6%9E%84/</link>
      <pubDate>Sat, 12 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/05-12-%E4%BB%A3%E7%A0%81%E9%87%8D%E6%9E%84/</guid>
      <description>重构自己的代码是一件幸福的事，重构别人的代码确是一件不幸的事。尤其是被重构代码的人还没有离职的时候&amp;hellip;
重构切记引入意外复杂度，同时要保证代码功能单一，没有冗余的、意义不明的代码存在。还要保证最简API原则。一个接口好坏，一定要明确区分：它不能再处理更多业务了，还是这些业务已经不能再被剥离了。
临时变量 这是重构的一个重要切入点。方法体内临时变量太多，尤其是一些词不表意的临时变量，理解维护是一件非常痛苦的事情。
是否可以将临时变量直接替换成方法调用，或者将临时变量规整到一个粒度更小的方法体中。
重复 代码重复，多处copy相同的代码，会让人迫不及待的想要重构。
项目中重复代码过多，会给维护、开发带来很大的不便。一个简单的逻辑修改，但却涉及修改好多处代码，还不能保证涉及的部分都修改了。这确实是一件头疼的事。
所以代码要做好封装：
 封装的粒度要把握好。一方面便于测试需要，另一方面通过组合，还能满足其他需求。 封装要考虑参数如何传递。包括是否应该包含成员变量，参数的个数多少合适。 封装的方法应该如何归类。怎样可以将方法归到最合适的类。  多态 当遇到很多的switch或者if-elseif的时候，可以考虑是否能用多态来替换。
比如下面的方法：
switch type { case movie.TV: case movie.Release }  需要特别提醒：movie包的常量作为判断条件，该方法就最好应该在movie包中。这样当需求变更时，便体现出最小修改原则。
我们提取一个movieType的抽象类，然后依次对每个类型实现相应的方法，通过声明类型为movieType的成员变量，实现不同类型的统一调用。
这里体现的是模块化的思想。将系统拆分成独立的模块，降低耦合度。这样做的好处：便于扩展。当新的类型添加时，对老的业务来说：零干扰。
但某些情况下，这样的拆分模式可能会有小缺陷。当类型是一个频繁被添加、修改的参数时，这样的模式就显得很冗余。这时可以使用属性拆分。将各个类型中的属性实现多态，也可以称做是一种策略转移。
使用类替换枚举类型 当一个类内含有多个常量枚举类型时，可以考虑将枚举类型的值封装成新的对象。这也是一个切入点。举个例子：
// original code const( Month int = iota Year int ) //modified code //将这些常量封装到另一个package中  访问成员变量 两个分歧：直接访问VS间接访问。两者均有好处，</description>
    </item>
    
    <item>
      <title>MySQL使用总结(一)</title>
      <link>/blog/2018/05-09-mysql%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93%E4%B8%80-/</link>
      <pubDate>Wed, 09 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/05-09-mysql%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93%E4%B8%80-/</guid>
      <description>查询的执行时间 第一次遇到查询时候报超时。很好奇，别的工具是如何修改查询的操时时间。
set max_statement_time = 0;  By using max_statement_time, it is possible to limit the execution time of individual queries.
 The MySQL version of max_statement_time is defined in millseconds, not seconds. MySQL&amp;rsquo;s implementation can only kill SELECTs.  left join 这个语句执行起来特别的费劲，但需求是：找出A表中存在，但B表中不存在的记录。
on条件 一直以为on是在执行表关联时的判断逻辑，即两个表的记录要不要关联，全靠on。直到遇到left join。发现它完全没有理会on提供的左表过滤条件，它返回了左表的全部记录，需要将条件放到where中才生效。
举个例子
-- table_a.id &amp;gt; 2018 无效 select * from table_a left join table_b on table_a.id = table_b.a_id and table_a.id &amp;gt; 2018 -- 正确的方式 select * from table_a left join table_b on table_a.</description>
    </item>
    
    <item>
      <title>Go test基础用法</title>
      <link>/blog/2018/05-02-go-test%E5%9F%BA%E7%A1%80%E7%94%A8%E6%B3%95/</link>
      <pubDate>Wed, 02 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/05-02-go-test%E5%9F%BA%E7%A1%80%E7%94%A8%E6%B3%95/</guid>
      <description>当直接使用IDE进行单元测试时，有没有好奇它时如何实现的？比如GoLand写的测试用例。
 所有的代码都需要写测试用例。这不仅仅是对自己的代码负责，也是对别人的负责。
最近工作中使用glog这个库，因为它对外提供的方法都很简单，想封装处理一下。但却遇到了点麻烦：这个包需要在命令行传递log_dir参数，来指定日志文件的路径。
所以，正常运行的话，首先需要编译可执行文件，然后命令行指定参数执行。如下示例：
go build main.go ./main -log_dir=&amp;quot;/data&amp;quot; //当前目录作为日志输出目录  但在go test的时候，如何指定这个参数了？
调查发现，发现go test也可以生成可执行文件。需要使用-c来指定。示例如下：
go test -c param_test_dir //最后一个参数是待测试的目录  执行后就会发现：这样的做法，会运行所有的Test用例。如何仅仅执行某一个测试用例了（编译器到底是如何做到的？）。
这里有另一个属性-run，用来指定执行的测试用例的匹配模式。举个例子：
func TestGetRootLogger(t *testing.T) { writeLog(&amp;quot;测试&amp;quot;) } func TestGetRootLogger2(t *testing.T) { writeLog(&amp;quot;测试2&amp;quot;) }  当我在命令行明确匹配执行Logger2，运行的时候确实仅仅执行该测试用例
go test -v -run Logger2 ./util/ //-v表示verbose，输出相信信息  但是，我发现，在指定了c参数之后，run参数无法生效！这样的话，还真是没有好的办法来处理这种情况。
Benchmark测试 关于如何运行Benchmark测试，默认执行go test并不会执行Benchmark，需要在命令行明确加上-bench=标记，它接受一个表达式作为参数，匹配基准测试的函数，.表示运行所有基准测试。
go test -bench=. // 明确指定要运行那个测试，传递一个正则表达式给run属性 go test -run=XXX -bench=.  默认情况下，benchmark最小运行时长为1s。如果benchmark函数执行返回，但1s的时间还没有结束，b.N会根据某种机制依次递增。可以通过参数-benchtime=20s来改变这种行为。
还有一个参数：benchmem。可以提供每次操作分配内存的次数，以及每次操作分配的字节数。
go test -bench=Fib40 -benchtime=20s  覆盖率 跟执行go test不同的是，需要多加一个参数-coverprofile,所以完整的命令：</description>
    </item>
    
    <item>
      <title>Saga Pattern</title>
      <link>/blog/2018/04-24-saga-pattern/</link>
      <pubDate>Tue, 24 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/04-24-saga-pattern/</guid>
      <description> 在微服务中，用的比较多的分布式事务模式：SAGA。
插播：在你觉得英文很难读懂的时候，别人却只是觉得有些英文论文很难读懂。所以，有时间就看一点这篇论文，总会看完的。
saga是一个本地事务的序列，每个事务都在各个微服务内部完成。通过外部的请求来开始第一个事务，且当前面的事务完成后，后面的事务就会被触发。
简要描述一下：
# 中心系统 - 充当分布式事务管理中心 1. 请求 - 订单服务 - 用户下单 2. 请求 - 库存服务 - 减少库存  下面介绍实现saga最流行的两种方式：
 Events 不需要一个中心调度系统，每个服务生产、监听别的服务产生的事件，决定下一步怎么处理。 Command 有一个中心服务来协调管理业务逻辑，做saga决策。  Events 在Events的方式中，各个服务执行完成事务之后，会发布一个event。其他服务会监听这个event，然后执行自己本地的事务，发布一个新的event。
当最后一个服务执行了本地事务，没有发布新的event,或者发布了其他服务不监听的event。分布式事务终止。
对这种方式来说，有效跟踪事务的执行状态是一个痛点。但实际工作中，确实需要明确知道事务的执行路径。两种解决办法：
 每个服务都更新当前的事务记录，记录可以存储在DB中，有几个服务，记录就应该有几个状态。 插入一个服务，监听所有服务的event。  补偿逻辑 分布式事务执行过程中，当其中一个事务执行失败之后，事务需要触发补偿逻辑。其原理还是发送一个event，只不过其他服务监听到之后，处发事务的补偿逻辑，回滚之前的本地操作。
实现方式 采用消息队列实现，以NSQ为例，可以这样考虑：
 event应该有一个唯一的身份标识。 每个服务至少应该监听一个topic，且至少作为一个topic的producer。 可以考虑一个统一的topic，多个channel的实现方式。  总结 当服务比较多的时候，topic可能会有很多，程序复杂性提高了不少。而且，一个留神，很可能让监听变成了一个死循环。
command </description>
    </item>
    
    <item>
      <title>docker基本使用</title>
      <link>/blog/2018/04-20-docker%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Fri, 20 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/04-20-docker%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</guid>
      <description>知道一点比完全不知道要好，对问题有深入了解比仅知道皮毛要好。作为docker的一个初学者，现在对docker做简单记录。希望随着工作、生活，更深入的了解学习docker。这也是一件很有意义的事。
docker有几个相关的概念：
 image 镜像 container 容器  我觉得之所以说docker好用，是因为Docker Hub提供了很多镜像，比如MySQL、Redis等。对它们安装、卸载异常方便。
下面举个例子，我们想搭建测试服务，安装MySQL，Redis等依赖。我们将他们当作一个项目的依赖，声明一个配置文件·db.yml，然后将这些依赖，类似于composer编辑：
version: &amp;quot;3&amp;quot; services: db: image: mysql:5.7 volumes: - /Users/neojos/dockerData/mysql restart: always environment: MYSQL_ROOT_PASSWORD: paytest MYSQL_DATABASE: paytest MYSQL_USER: neojos MYSQL_PASSWORD: neojos-pwd ports: - &amp;quot;3306:3306&amp;quot; myredis: image: redis restart: always volumes: - /Users/neojos/dockerData/redis ports: - &amp;quot;6379:6379&amp;quot; command: redis-server --appendonly yes  执行如下命令，MySQL和Redis的服务就启动了
docker-composer -f db.yml up  可以通过执行如下命令查看，确认是否有两个容器在运行。
docker container ls  这样很好，但当我想进去MySQL的容器内执行一些命令时，该怎么办呢？比如，我想确认下面的MySQL连接语句是否正确,而且我还一定要进去容器内执行MySQL命令行语句：
mysql -h 127.0.0.1 -P 3306 -u neojos -p&#39;neojos-pwd&#39; paytest  很简单,只需要执行如下指令。可以发现，已经进到MySQL命令行了。</description>
    </item>
    
    <item>
      <title>xorm使用reverse指令创建模版</title>
      <link>/blog/2018/04-19-xorm%E4%BD%BF%E7%94%A8reverse%E6%8C%87%E4%BB%A4%E5%88%9B%E5%BB%BA%E6%A8%A1%E7%89%88/</link>
      <pubDate>Thu, 19 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/04-19-xorm%E4%BD%BF%E7%94%A8reverse%E6%8C%87%E4%BB%A4%E5%88%9B%E5%BB%BA%E6%A8%A1%E7%89%88/</guid>
      <description>这只能算作一次小的功能介绍
结合我们使用过的go数据操作类的库，执行的逻辑基本都是：将数据库返回的数据，转换成我们提前声明的结构体对象，然后返回。
今天要介绍的就是如何自动创建每个table对应的结构体。
查看 xorm tool的介绍：
xorm reverse mysql root:@/xorm_test?charset=utf8 templates/goxorm  初看这个介绍，让我费了一段时间才理解。你可以在命令行查看它的具体含义：
xorm help reverse  命令中templates/goxorm其实是xorm提供好的模版路径。我错误的理解成了：执行命令生成结果的存储路径。
tmplPath Template dir for generated. the default templates dir has provide 1 template  其次就是mysql的连接语句：一般来说，都是这样写的：
username:pwd@ip:port/db?charset=utf8  但是使用上述方式却无法正常执行命令，正确的方式是：
xorm reverse &amp;quot;username:pwd@tcp(ip:port)/db?charset=utf&amp;quot; templates/goxorm  </description>
    </item>
    
    <item>
      <title>了解Laravel依赖注入</title>
      <link>/blog/2018/04-05-%E4%BA%86%E8%A7%A3laravel%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5/</link>
      <pubDate>Thu, 05 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/04-05-%E4%BA%86%E8%A7%A3laravel%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5/</guid>
      <description>随笔 突然想了解一下Laravel，然后发现：它没有我想象的那么简单，很多的调用都找不到入口。加载view的逻辑，看了很长时间，还是没有搞明白：一个值传递的参数，怎么好好的就变了呢？下面都是看别的的文章的总结，我还要继续完善，直到搞清楚这个view是怎么实现的。
看了两篇文章，介绍了如何使用xdebug断点调试php及测试性能。作为了解Laravel的必要工具，也介绍进来。
 How to Install Xdebug with PHPStorm and Vagrant Debugging and Profiling PHP with Xdebug  概要 使用use Illuminate\Container\Container;作为参考的例子。
可以浏览原创：Laravel Container (容器) 深入理解 (下)。
摘抄Laravel
 The Laravel service container is a powerful tool for managing class dependencies and performing dependency injection.
 通过config/app.php可以查看Laravel的Service container。Service下的register便是用来创建binding的。通过php artisan make:provider CustomServiceProvider创建自定义的ServiceProvider。
 There is no need to bind classes into the container if they do not depend on any interfaces.</description>
    </item>
    
    <item>
      <title>包管理工具</title>
      <link>/blog/2018/03-31-%E5%8C%85%E7%AE%A1%E7%90%86/</link>
      <pubDate>Fri, 30 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/03-31-%E5%8C%85%E7%AE%A1%E7%90%86/</guid>
      <description>反思之前的过程，一直没有试图跟上技术的发展。恍然觉得，其实技术比买股票更能让我找到快乐。新的技术越来越多，能做的便是，持续保持蜗牛锲而不舍的精神，慢慢爬！
首先感谢这篇文章2018 年了，你还是只会 npm install 吗？，让我重新开始审视包管理工具。因为在PHP开发中有Composer，在Go的开发中有glide。但却没有尝试思考它们背后的那些为什么。
npm包管理 我一直不理解package.json和package-lock.json这两个文件的作用。直观上看，前者是我们项目所依赖的包，后者是各个包自身的明细依赖。但这样的设计却是经过多个版本迭代最终确定的形式。
当我们执行install或者update的时候，package-lock.json会根据nodemodules的更新而进行相应更新。当前就理解到这里，请看Composer
包的版本 包的版本号采用semver约束，由3个数字组成，格式必须为 MAJOR.MINOR.PATCH, 意为： 主版本号.小版本号.修订版本号。
约束还有一条：主版本号相同的升级版本必须提供向下兼容，但这仅仅是口头约束。测试版本的匹配，可以访问网址：https://semver.npmjs.com/。
 ^开头的版本：主版本号相同，大于等于小版本号的所有版本。 ~开头的版本：主版本、小版本号相同，大于等于修正版本的版本。 *或者x的版本：两者表示通配符。 在常规仅包含数字的版本号之外：表示不稳定的发布版本。  管理依赖 有时候，项目和项目之间存在引用依赖关系。比如将多个项目间共同使用的类在common项目下维护，然后其他项目project-1和project-2分别引用项目common。当project项目变得越来越多时，每次新的项目都需要手动拷贝common代码。
可以将common做为一个包来管理。创建package.json文件，将common项目托管到git仓库。执行npm install git_url就可以将common作为依赖包进行安装了。
npm除了安装git仓库的代码，也可以安装本地的代码。
npm install file:local-package-path  版本管理 svn或者git只需要提交package.json, package-lock.json, 不需要提交node_modules目录。
每次升级或降级版本，执行如下代码，相应的package.json，package-lock.json会自动更新：
npm install &amp;lt;package-name&amp;gt;@&amp;lt;version&amp;gt;  删除依赖包：
npm uninstall &amp;lt;package&amp;gt;  Composer管理 Composer生成的包管理目录叫vendor，它也是生成两个文件composer.lock和composer.json。composer.lock描述了项目的依赖以及其它的一些元信息。
composer.lock用来明确锁定安装包的具体版本信息，包证所有人安装的版本都是一致的。具体的原因在于：
 composer.json中指定的安装包版本，比如^2.0，只能确定该包的主版本号一定是2，当Composer在install的过程中，具体安装了该包符合条件的哪个版本，是无法从.json中看出来的。 同理，还是上面的例子，如果一个同事，数月前执行install安装的版本是2.0.0，后来这个包在2版本下发布了一个小版本2.1.0。另一个同事后来执行install，很可能就安装成了2.1.0  综上所述，composer.lock用来保证安装包的一致性，避免安装到不同的版本包，给生产环境带来的不确定性。
install/update install主要用来安装新包。当安装新包的时候，需要首先查看.lock文件是否存在，如果存在，安装.lock中指定的具体版本。如果不存在，直接安装。同时更新.json和.lock两个文件。
update主要用来更新.lock中安装的包。随着时间的推移，.json中的包可能又发布了新版本，所以update就是用来检查.json中包的新版本，更新.lock文件用的。
我在使用的过程中，比较倾向于使用下面的单个包操作的方式：
php composer.phar update monolog/monolog [...]  版本管理 在git环境中.json和.lock都需要被提交的版本控制。vendor目录就不需要啦。
glide版本管理 glide是go的版本管理工具。其实glide也是参考composer设计的，所以上面对composer的说法也同样有效。
在项目开发中也仅需要对 glide.yaml 和 glide.</description>
    </item>
    
    <item>
      <title>HTTP总结-状态码</title>
      <link>/blog/2018/03-17-http%E6%80%BB%E7%BB%93-%E7%8A%B6%E6%80%81%E7%A0%81/</link>
      <pubDate>Sat, 17 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/03-17-http%E6%80%BB%E7%BB%93-%E7%8A%B6%E6%80%81%E7%A0%81/</guid>
      <description>给别人轻松讲明白一个问题，才能算自己真正了解这个问题。 &amp;gt; Origin Header 头让我熟悉了一次sheme
从HTTP的头Origin说起，想起之前客户端定义scheme，因为不了解，问了开发的同事“scheme是什么？”反正我当时是不明白他们讲的。
在了解HTTP Origin语法的时候，我其实才真正明白：scheme 指请求所使用的协议，通常是HTTP、HTTPS或者其他。
Origin: &amp;lt;scheme&amp;gt; &amp;quot;://&amp;quot; &amp;lt;host&amp;gt; [&amp;quot;:&amp;quot; &amp;lt;port&amp;gt;]  Origin表示请求来至哪个站点。在WebSocket通信的时候，明确指明要校验这个参数。
405 方法不被允许 (Method not allowed)。用来访问本页面的谓词不被允许，有时将POST请求修改为GET请求之后异常就解决了。
比如：Web端通过Ajax异步提交数据，并且是POST的方式。莫名奇妙的的发现返回的状态码是405。很有可能是服务端在处理请求时出错了，在Nginx返回时，返回了404.html或者500.html导致的。
500 服务器内部错误。比如：服务端处理出现异常。同时，在PHP错误日志中可以查看异常发生的调用栈信息。
502 作为网关或者代理工作的服务器尝试执行请求时，从上游服务器接收到无效的响应。比如nginx从php-fpm接收到了不完整的response数据。
比如：服务端尝试连接mysql，但长时间链接不上，就会返回502错误。
可以浏览一些具体的文章：
 http 502 和 504 的区别 Nginx一次奇怪的502 报错探究  503 服务暂时不可用，一段时间后服务就可以正常工作了。
在代码发布的时候可能会使用到。比如go服务，在发版前需要将之前运行的进程kill掉，之后启动新的进程。但这个过程间隙，会导致已连接的客户端处理中断。所以在发版之前，先返回503,等待已经接收的请求处理完成，然后升级。
504 网关超时。为了完成您的 HTTP 请求，该服务器访问一个上游服务器，但没得到及时的响应
比如：nginx超过了自己设置的超时时间，不等待php-fpm的返回结果，直接给客户端返回504错误。但是此时php-fpm依然还在处理请求（在没有超出自己的超时时间的情况下）。
0、超时、客户端主动断开连接 If you connect with the server, then you can get a return code from it, otherwise it will fail and you get a 0.</description>
    </item>
    
    <item>
      <title>Nginx总结</title>
      <link>/blog/2018/03-01-nginx%E6%80%BB%E7%BB%93/</link>
      <pubDate>Sat, 10 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/03-01-nginx%E6%80%BB%E7%BB%93/</guid>
      <description>location 指令 工作中经常用到的一个指令，用来对某个路径的请求做特殊处理。比如同样的链接在PC和Web显示不同的页面。
location修饰符 location block匹配request url中domain name 或者ip/por之后的请求部分，即请求资源的路径。
形式如下：
location optional_modifier location_match { }  如下是optional_modifier的类型：
  ​ optional_modifier ​ 含义   ​  =  ​ 请求的url必须严格匹配被location指定的路径，必须完全相同   ​ none ​  如果没有修饰符，将对url做前缀匹配 ​   ​ ^~  ​ 最佳的非正则表达式前缀匹配    ​ ~  ​ 大小写敏感的正则匹配    ​ ~*  ​ 大小写不敏感的正则匹配    location匹配规则  nginx会查找一个精确匹配。如果匹配到了 = modifier，匹配会立即终止，该location就会被选择处理这个请求。 如果没有精确匹配（= modifier），nginx继续进行前缀匹配，对于给定的url，选择最长的前缀匹配。然后依据下列规则，继续匹配。 如果最长的前缀匹配有（^~ modifier），nginx会立即结束查询，选择该location。如果没有 ^~ modifier，该匹配会被暂时存起来，以便搜索可以继续。 最长的匹配被存起来后，nginx会继续匹配正则表达式。nginx移动到 location list 的顶部，然后试着去匹配正则表达式，第一个被匹配的正则表达式会立即被选择处理请求，结束匹配。 如果没有正则表达式被匹配，则之前存储的最长location被选择用来处理请求。  特别需要理解的：nginx正则匹配结果优先于前缀匹配。但是前缀匹配在先，同时允许通过 ^~ 和 = 来改变这种趋势。</description>
    </item>
    
    <item>
      <title>IAP支付初识</title>
      <link>/blog/2018/02-08-iap%E6%94%AF%E4%BB%98%E5%88%9D%E8%AF%86/</link>
      <pubDate>Wed, 28 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/02-08-iap%E6%94%AF%E4%BB%98%E5%88%9D%E8%AF%86/</guid>
      <description>IAP全称In-App Purchase，也可以叫内购。查看百度百科，IAP是一种智能移动终端应用程序的付费模式。大概的意思：用户在APP内通过付费，来享受APP内提供的服务或体验。
我不仅仅想总结一下苹果的IAP，还想反思一下支付要注意的细节。
从问题入手：如何确认苹果交易的唯一标识。想要做到支付的幂等性，每一笔订单都应该有一个唯一的标识。来避免出现类似这样的现象：用户支付了一次，服务端却创建了多个订单。
交易的唯一标识 我们使用服务端校验支付流程，每笔交易都通过服务器请求苹果服务器来完成校验。
IAP支付的流程  苹果IAP支付有一个“事务”的概念。当用户支付完成，苹果会回调APP，传递一个receipt的凭证。 APP端本地校验receipt或者APP回传到自己Server端对其校验 校验通过后，APP端主动finish调该transaction  transaction理解 对于每一次支付，都会产生一个新的transaction，用来唯一标识该订单。客户端每次finish的对象也是它。
对于它是不是唯一的疑问，查阅了部分文档，有很多订阅型的产品的开发反馈：transaction在一段时间后可能会发生变化。但我查询的结果认为：transaction可以唯一确定一笔交易。
如下摘录苹果论坛的一段描述：
 There are two transactionIdentifiers - the one that comes with the particular purchase and the one in the purchase receipt. Any call to updatedTransactions, including the call when you originally purchase the IAP, has a transaction.transactionIdentifier that is always unique. When you originally purchase an IAP or when you repurchase an IAP for free or when you restore an IAP the receipt will also contain the &amp;ldquo;unique&amp;rdquo; transaction_id of the original purchase transaction.</description>
    </item>
    
    <item>
      <title>WebSocket基础开发</title>
      <link>/blog/2018/websocket%E5%9F%BA%E7%A1%80%E5%BC%80%E5%8F%911/</link>
      <pubDate>Sun, 25 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/websocket%E5%9F%BA%E7%A1%80%E5%BC%80%E5%8F%911/</guid>
      <description>WebSocket是一种网络通讯协议。在服务器端可以将HTTP请求升级为WebSocket请求。区别于普通的HTTP请求，WebSocket中存在特殊的字段标识：
GET /chat HTTP/1.1 Host: server.example.com Upgrade: websocket Connection: Upgrade Sec-WebSocket-Key: x3JJHMbDL1EzLkh9GBhXDw== Sec-WebSocket-Protocol: chat, superchat Sec-WebSocket-Version: 13 Origin: http://example.com  这种协议升级，是在应用层实现的。所以一个服务器本身既可以提供WebSocket服务，也可以提供正常的HTTP服务。
我们下面对服务做区分。/ws负责对外提供WebSocket服务。
http.HandleFunc(&amp;quot;/&amp;quot;, serveForHttp) http.HandleFunc(&amp;quot;/ws&amp;quot;, serveForWs)  将HTTP请求升级为WebSocket请求,处理连接的读写操作：
func serveForWs(w http.ResponseWriter, r *http.Request) { if r.Method != &amp;quot;GET&amp;quot; { http.Error(w, &amp;quot;Method not allowed&amp;quot;, http.StatusMethodNotAllowed) return } conn, err := upgrader.Upgrade(w, r, nil) if err != nil { log.Println(err) return } go client.write() go client.read() }  Upgrader负责连接升级，同时指定连接的部分属性。包括ReadBufferSize，WriteBufferSize，CheckOrigin等。Upgrader默认会检查header头中Origin是否有效，如果你要使用这个默认函数的话，需要确保客户端请求头中包含Origin。
var upgrader = websocket.</description>
    </item>
    
    <item>
      <title>Protobufs在Logstash中的应用</title>
      <link>/blog/2018/protobufs%E5%9C%A8logstash%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</link>
      <pubDate>Tue, 30 Jan 2018 20:10:33 +0000</pubDate>
      
      <guid>/blog/2018/protobufs%E5%9C%A8logstash%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</guid>
      <description>ELK 分别是Elasticsearch、Logstash、Kibana技术栈的结合。主要解决的问题在于：系统服务器多，日志数据分散难以查找，日志数据量大，查询速度慢，或者不够实时。
 在trivago，我们主要依靠ELK来处理日志。我们通过使用Kafaka，将服务器的访问日志、错误日志、性能基准数据及各种各样的诊断日志，传递给Logstash，Logstash处理之后将日志存放到Elasticsearch。在数据传输中，我们更倾向使用protocol buffer对数据进行编码。这篇博客，我们将主要介绍如何使用Logstash来解析protobuf编码的消息。 相比无模式的JSON格式，Protobufs是一种有模式、高效的数据序列化格式。我们在Kafaka中传输的数据，很多都在使用Protocol Buffers进行编码。它的优势就在于：首先，编码后的数据size明显要比其他的编码方式要小。以JSON编码举例，消息体中不仅仅包含实际数据，还有对应的Key值及很多的中括号。对于文档结构基本不变的数据，传输中包含这些附加信息，是一种资源的浪费。当发送端和接收端对交互的文档结构达成一致后，传输过程还携带这部分结构信息就显得多余。在整个日志处理过程中，该部分消耗的资源是可以被节省下来。其次，消费者所处理的数据，数据格式都是约定好的，完全不会像JSON一样，莫名奇妙多出一个字段。同时，给数据字段的理解产生误解。
不开心的是，Logstash不支持Protobufs编解码。目前，它支持纯文本、JSON格式和其他别的消息格式。因此，我们决定自己实现这部分功能。
如何写Logstash的编码器 写一个Logstash插件是相对容易的，你只需要掌握一些基本的Ruby知识。Ruby语言天生直观简单，你很可能在查看示例代码的同时，就将它学会了。对初学者而言，tryruby.org是很不错的学习网站。你需要在电脑上安装Jruby，其他环境部分请参照elastic&amp;rsquo;s documentation。当你发现github上codec项目是空的时候，请不要困惑，请你clone JSON codec或plain codec来代替。通过这个过程，你将会了解到现存的插件是如何开发的，同时，你还能掌握ruby的相关知识。
校验JAVA环境变量是否设置成功
java -version  获取protobufs 最后，你下载logstash-codec-protobuf插件来解码protobuf消息。要使用这个插件，你需要一些proto文件和使用该proto格式编码的数据。如果这个proto文件已经在别的工程中使用了，那么你就仅仅需要创建proto文件的Ruby版本。如果这完全是一个新的项目，那么你可能需要先从Google&amp;rsquo;s developer pages了解一下proto 文件的语法规则，找到适合你项目的工具链，然后编译proto文件。
列举一个proto文件的sample，使用Go编码：
//使用proto3， 不支持optional选项 syntax = &amp;quot;proto3&amp;quot;; package tutorial; message Person { string name = 1; int32 id = 2; // Unique ID number for this person. string email = 3; }  安装插件 从rubygems下载logstash的插件，执行如下命令：
bin/plugin install PATH_TO_DOWNLOADED FILE  这个解码器支持Logstash 1.x和2.x版本。
创建Ruby版本的protobufs文件 假设下面的 unicorn.pb文件是我们定义的proto文件，我们将要使用它去解码消息：
package Animal; message Unicorn { // colour of unicorn optional string colour = 1; // horn length optional int32 horn_length = 2; // unix timestamp for last observation optional int64 last_seen = 3; }  下载ruby-protoc的编译器，然后运行:</description>
    </item>
    
    <item>
      <title>RequireJS初识</title>
      <link>/blog/2018/requirejs%E5%88%9D%E8%AF%86-/</link>
      <pubDate>Mon, 29 Jan 2018 20:10:33 +0000</pubDate>
      
      <guid>/blog/2018/requirejs%E5%88%9D%E8%AF%86-/</guid>
      <description>开发上想找一个时间选择控件，无意中就找到了layDate 日期与时间组件。
直接下载源码，在文件中引入.js和.css文件，但是调用的时候产生异常了。很好奇！作为一名服务端开发，我一直都是这样搞的，百试不爽。今天却翻车了！！查看错误发现是require这个方法报的错，主要的原因是：laydate未定义。
为什么要引入requirejs，这个东西到底该怎么用呢？从后端的角度看：它其实就是扮演PHP中的 spl_autoload_register的角色。当执行JS的时候，自动去调用执行脚本所需的js。
所以对代码做如下修改,主要用来配置加载js的路径。
注：如果是本地资源，千万不要写成如下代码所示的：域名+路径的形式（见注释掉的baseUrl）。当正式服和测试服域名不相同时，就比较麻烦。我就是因为我们测试服的域名是woniu-test，正式服我的域名是woniu，结果require的加载请求一直请求的是woniu的域名，找了半天才发现这个问题。
&amp;lt;script&amp;gt; requirejs.config({ //baseUrl: &#39;http://woniu/resource&#39; baseUrl: &#39;/resource/&#39;, //paths: { // laydate: &#39;js/laydate&#39; //}, //shim:{ // &#39;laydate&#39;: { // deps: [&#39;js/laydate&#39;], // exports: &#39;laydate&#39; // } //} }); &amp;lt;/script&amp;gt;  调用的时候直接如下调用，就ok了！！
&amp;lt;script&amp;gt; require([&#39;js/laydate&#39;], function (_){ // called once the DOM is ready laydate.render({ elem: &#39;#input-end-time&#39;, //指定元素 type: &#39;datetime&#39; }); laydate.render({ elem: &#39;#input-start-time&#39;, //指定元素 type: &#39;datetime&#39; }); }); &amp;lt;/script&amp;gt;  </description>
    </item>
    
    <item>
      <title>Memcached遇到的json_decode问题</title>
      <link>/blog/2018/memcached%E9%81%87%E5%88%B0%E7%9A%84json_decode%E9%97%AE%E9%A2%98/</link>
      <pubDate>Sun, 21 Jan 2018 15:48:33 +0000</pubDate>
      
      <guid>/blog/2018/memcached%E9%81%87%E5%88%B0%E7%9A%84json_decode%E9%97%AE%E9%A2%98/</guid>
      <description>Memcached 是一个高性能的分布式缓存系统，使用Key-Value存储字符串和对象。通常来说，它主要用于缓存从数据库中检索到的数据以及第三方服务的数据等。简单的说，它可以提升服务器的性能。几乎所有的程序语言都可以接入它的API。如下例子所示：
public function getYouData(string $key) { $yourData = $memcached-&amp;gt;get($key); if (!$yourData) { $yourData = $yourDb-&amp;gt;getAll(); $memcached-&amp;gt;set($key, $yourData); } return $yourData; }  在trivago， 我们使用Memcached做缓存层，而且我们对外仅提供缓存接口。开发过程中，程序员不需要考虑缓存的内部实现，仅仅知道如何调用接口就可以了。目前，该API在PHP的代码库中几乎都有使用。我们使用Memchached的场合已经相当多了，随着每次新版本发布，使用量还在增加。
一天，系统日志文件里几乎全是Memcached的报错，get方法调用失败，导致所有的请求直接打到了数据库上。当然，在巨大负载的情况下，这些请求最终也失败了。最终，我们遇到了影响trivago整个平台能正常运行的问题。
那么到底发生了什么？为什么Memcached开始出问题了？
 Botnet也就是我们所说的僵尸网络，是指采用一种或多种传播手段，将大量主机感染bot程序（僵尸程序），从而在控制者和被感染主机之间所形成的一个可一对多控制的网络。
 原因是来至于200个国家，70K独立IP的网络攻击，直接导致当时负载飙升到平时的40倍。10分钟后我们的蛛网节流机制被触发，攻击的影响被慢慢减弱。
攻击造成Memcache的网络带宽饱和，直接原因是其中一个库的get/hit请求引起的。查看发现，这个库已经使用了大约4GB内存。很明显，这里有一些问题。
之后，我们对缓存记录了更加详细的日志。当然，我们之前也记日志，只是无法从现有日志中发现，究竟是哪些key消耗了大部分内存。因此，我们特别对value的占用内存大小做了额外的记录。
一天后，通过log记录，我们终于发现了这‘怪物’：ItemRepository 下的静态方法getAllItemData ，缓存的数据平均有10M左右。
仅仅只是名字，听起来就怪吓人的吧？更可怕的是，这个方法是2014年写的，从2015年起就再也没有被改动过。根据Blackfire的性能剖析，每加载一个页面就会调用30次getAllItemdata 方法。
接下来，我们单独对这个方法debug，为什么缓存的值会这么大？结论是：我们正在使用默认的Memcached serialization方法，更精确的讲，是原生的PHP serialize / unserialize方法（自从我们迁移到PHP7，我们就停止使用igbinary 的扩展，因为两者结合的时候会出现问题，因此序列化的工作又重新交给了php）。这也就意味着除了存储必要的数据之外，还需要额外存储对象的类名、属性等信息。
问题很明显了，这调整起来应该也非常简单。将php serizlization 成某种更加紧凑的数据存储格式。
当前的环境，使用igbinary的话，改动会非常大。因此我们考虑使用JSON或Protobuf，但是基于灵活性、快速实现的考虑，我们最终决定使用json，它是一种简单、轻量的数据存储格式。
JSON是无模式的数据结构，对数据进行编码非常方便，但解码的时候，需要将数据映射到对应的类上。
//people是一个类，json_encode不会编码对象的私有变量 $zhangsan = new people(&#39;zhangsan&#39;, &#39;boy&#39;, &#39;23&#39;) $json = array(&#39;people&#39; =&amp;gt; $zhangsan); $jsonEncode = json_encode($json); $jsonDecode = json_decode($jsonEncode);  我们考虑是否要使用一个外部的扩展：Symfony组件Serializer，然而经过一系列基准测试之后，我们还是决定手动实现数据编码和对象的映射关系。主要还是出于对PHP性能的考虑，对我们而言，手动实现也仅仅只是额外调用一次内部实例对象，并且，我们还可以灵活的对它进行调整。
 实现json_ecode方法，编码从数据库检索到数据 改变缓存中key的前缀，确保跟之前的不存在冲突 增加json_decode方法，用于从Memcached中获取数据 将数据转换成对应的PHP实体或对象  听起来很简单，是吧？但我们运行测试时，json_decode持续地返回错误：语法错误、控制字符错误、或者错误的UTF-8格式。</description>
    </item>
    
    <item>
      <title>Git分支模型</title>
      <link>/blog/2018/git%E5%88%86%E6%94%AF%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Sun, 14 Jan 2018 20:10:33 +0000</pubDate>
      
      <guid>/blog/2018/git%E5%88%86%E6%94%AF%E6%A8%A1%E5%9E%8B/</guid>
      <description>Git分支模型 文章将围绕下图来描述我们所使用的分支模型。主要包括master和develop两个主线分支以及feature、release、hotfixes分支。
为什么选择Git 针对“centralized”和“distributed”版本管理工具的争论，可以在GitSvnComparsion查看。就我个人而言，我更喜欢Git。Git改变了开发者对merge操作和branch操作的思考方式，而且两者也是Git日常工作流中的最常用的操作。
不集中式又集中式 Git是分布式版本管理系统，不存在集中式版本管理系统的中央存储库。这在技术角度上确实不存在，但在观念上，我们可以将origin看作整个版本管理的中央存储库。
如下图所示，开发者除了可以从origin中push或pull代码，还可以从别的分支中pull代码。当多个同事共同开发产品的新功能时，彼此间的代码同步显得尤为重要。
主要分支 Git中央存储库中包含两个重要的分支，它们在项目的生命周期中都一直存在：
 master develop  两个分支有如下特性：
 origin/master分支的HEAD 指针反映的一直都是发布就绪的状态。master分支上的代码也是生产服务的代码。 origin/develop分支的HEAD指针反映当前项目的修改，该分支集成其他分支所做的一切修改。甚至可以运行一个自动化脚本，每天晚上将各个分支的修改merge到develop分支。  当develop分支中的代码趋于稳定，准备发新版的时候，应该将其merger到master分支，并标记本次发布的版本号。稍后详细讨论。
原则上，master分支的代码都是可发布的，所以我们对merge到master的代码有严格的要求。理论上，我们可以运行一个脚本，一旦检测到master的代码有提交，自动执行编译、并同步代码到生产服务器。
支承分支 如master和develop旁边的其他分支，它们的生命周期有限，最终会从代码库中被移除。而我们使用分支主要来实现：
 来帮助各个团队之间并行开发 为新版本发布做准备 修复当前生产环境的bug。  我们使用的分支有以下几种:
 Feature branches Release branches Hotfix branches  各个分支根据不同的目的被创建，对它们的操作也遵循严格的规则。比如分支如何创建、开发完成之后merge到的对象等。
另外，这些分支其实都是普通的git分支。只是根据我们使用的目的策略给他们赋予了不同的功能。
Feature 分支 Feature 分支主要用来开新功能。一般来说，只要功能还没有开发完善，它就应该一直存在。但最终应该被merge回develop分支或者丢弃。feature分支遵循以下规则：
 从develop分支上创建feature分支 feature分支最终merge回develop分支 分支的命名规则：除了master, develop, release-*, or hotfix-*的任何名字  feature分支通常只存在于开发人员的版本库中，而不应该存在于origin仓库中。但考虑到团队成员协作开发的情况，彼此之间需要定期merge对方的代码，这是就需要借助develop分支来实现了。
创建feature分支 git checkout -b myfeature develop  合并feature 分支 git check develop git merge --no-off myfeature git branch -d myfeature git push origin develop  release分支 release分支主要用来为代码发布做准备。在合并代码之前，它允许做小的bug修改、为版本发布做准备工作（指定版本号、建数据表等）。通过在release分支上做这些操作，可以保证develop分支是干净的，不影响当前新功能的开发。release分支遵循下面的规则：</description>
    </item>
    
    <item>
      <title>Redis学习的惨痛经历</title>
      <link>/blog/2018/redis%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%83%A8%E7%97%9B%E7%BB%8F%E5%8E%86/</link>
      <pubDate>Sun, 31 Dec 2017 20:10:33 +0000</pubDate>
      
      <guid>/blog/2018/redis%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%83%A8%E7%97%9B%E7%BB%8F%E5%8E%86/</guid>
      <description>我们开发的产品类似于 trivago hotel search，Redis也多用来缓存临时数据。比如将操作频繁的流水数据先存储到redis，之后迁移到关系型数据库做持久化。
旅店查找的功能，前端主要是靠PHP和Symfony Framework开发，后端是Java。本章我们主要强调PHP和Redis的协作，目前它运行的非常稳定，但我们为实现这一步却花费了很大的精力。下面来说我们学习Redis的经历。
前言 起初我们使用的库是 Predis，一直到2013年我们开始使用phpredis (C实现)，主要因为二者的性能差异。
在2014年，我们给平台开发了新的特性，导致http 请求短时间内翻了一倍，结果有40%的请求HTTP 500: Internal Server Error.
之后查看日志，发现错误多是redis的连接问题： read error on connection 和 Redis server went away。
| WARN | ... Redis\ConnectException: Unable to connect: read error on connection ... #0 /.../vendor/.../Redis/RedisPool.php(106): ...\Redis\RedisPool-&amp;gt;connect(Object(Redis), Object(...\Redis\RedisServerConfiguration)) #1 /.../vendor/.../Redis/RedisClient.php(130): ...\Redis\RedisPool-&amp;gt;get(&#39;default&#39;, true) #2 /.../vendor/.../Redis/RedisClient.php(94): ...\Redis\RedisClient-&amp;gt;setMode(false) ... #17 /.../app/bootstrap.php.cache(551): Symfony\Bundle\FrameworkBundle\HttpKernel-&amp;gt;handle(Object(Symfony\Component\HttpFoundation\Request), 1, true) #18 /.../web/app.php(15): Symfony\Component\HttpKernel\Kernel-&amp;gt;handle(Object(Symfony\Component\HttpFoundation\Request)) #19 {main} | 12.34.56.78 | www.trivago.de | /?aDateRange%5Barr%5D=2014-05-20&amp;amp;aDateRange%5Bdep%5D=2014-05-21&amp;amp;iRoomType=1&amp;amp;iPathId=44742... | Mozilla/5.0 (WindowsNT 6.</description>
    </item>
    
    <item>
      <title></title>
      <link>/blog/2018/%E4%B8%B4%E6%97%B6%E4%BF%9D%E5%AD%98%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93sql/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/%E4%B8%B4%E6%97%B6%E4%BF%9D%E5%AD%98%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93sql/</guid>
      <description>保存生活作风请求体
select * from u_charge where c_uid = 10318003 order by c_id desc ; select * from u_charge where c_order = &amp;quot;1000000038797108&amp;quot;; select * from u_charge where c_channel_transaction_no = &amp;quot;1000000038797108&amp;quot;; delete from u_charge where c_uid = 10318003 order by c_id limit 50; select * from u_charge where c_id = 4046035 and c_status = 0;    </description>
    </item>
    
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>网站是使用hugo + GitHub搭建的静态网页。在这之前，一直用的是hexo，自从无意间看到了https://yihui.name/。不得不说，自己被博主的优秀以及博客的简单明了折服的一塌糊涂。
简单了解hugo之后，发现它更简单、更方便，所以将自己的博客也做了迁移。</description>
    </item>
    
  </channel>
</rss>