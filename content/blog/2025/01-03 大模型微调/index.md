---
title: 大模型微调
date: 2025-01-03
author: 付辉

---

第一句话：如果使用少量相关领域的标注数据对模型进行微调，它还可以适应具体任务的需求。

第二句话：在这种背景下，研究者可以使用大量的低成本、易获取的无标注语料对模型进行预训练。

问题，微调是什么？预训练是什么？它们是什么关系？

DeepSeek R1 的一个重要优势在于它能够将高级推理能力迁移到较小的模型中。DeepSeek 团队生成了 60 万条推理数据，在 Qwen 和 Llama 等开源模型上证明了这种迁移能力。即使不使用强化学习，直接从 R1 模型进行迁移也能实现强大的推理性能。

BERT模型在预训练任务中使用MASK符号掩码部分输入内容，但是这些符号在微调时并不会出现，从而导致预训练与微调之间存在差异。