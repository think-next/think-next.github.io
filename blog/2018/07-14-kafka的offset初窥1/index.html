<!DOCTYPE html>
<html>

<head>
    
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">

<meta name="description" content="Hugo-Primer theme&#39;s example site">
<title>
    Kafka的offset初窥(1) - 渐行渐远
</title>









<link rel="stylesheet" href="/css/main.min.ea2156633063385371d7a4d9db559868e643a3885ab887da32f7ea8a1e05ee11.css" integrity="sha256-6iFWYzBjOFNx16TZ21WYaOZDo4hauIfaMvfqih4F7hE=" crossorigin="anonymous" media="screen">





<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Didact+Gothic">
<script
        src="https://code.jquery.com/jquery-3.5.1.min.js"
        integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
        crossorigin="anonymous"></script>

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Kafka的offset初窥(1)"/>
<meta name="twitter:description" content="kafka适用的场景很多，但用它来异步通知却是让我略感头痛！ 引言 对于kafka的offset问题，先从这篇文章说起：How to disable auto commit? 它阐述了一"/>

<meta property="og:title" content="Kafka的offset初窥(1)" />
<meta property="og:description" content="kafka适用的场景很多，但用它来异步通知却是让我略感头痛！ 引言 对于kafka的offset问题，先从这篇文章说起：How to disable auto commit? 它阐述了一" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://neojos.com/blog/2018/07-14-kafka%E7%9A%84offset%E5%88%9D%E7%AA%A51/" />
<meta property="article:published_time" content="2018-07-14T00:00:00&#43;00:00"/>
<meta property="article:modified_time" content="2018-07-14T00:00:00&#43;00:00"/>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-12345678-0', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

    

    
    
    
    <title>
        
        Kafka的offset初窥(1)
        
    </title>
</head>

<body>

    
    
    <header class="text-center container">
        <h1 class="mb-4 mt-2">Kafka的offset初窥(1)</h1>
    </header>
    
    <main class="wrap">
        
<div class="container">
    <aside role="complementary">
        <div class="tag-container">
            
            
            <span class="tag">
                <a href="/tags/mq/" class="badge badge-success">
                    MQ
                </a>
            </span>
            
            
        </div>
        <div class="float-right badge badge-light">07-14-2018 &#183; 1497 words</div>
    </aside>
    <hr />
    <article role="article">
        

<p><code>kafka</code>适用的场景很多，但用它来<code>异步通知</code>却是让我略感头痛！</p>

<h2 id="引言">引言</h2>

<p>对于<code>kafka</code>的<code>offset</code>问题，先从这篇文章说起：<a href="https://github.com/bsm/sarama-cluster/issues/76">How to disable auto commit?</a> 它阐述了一个重要的信息：</p>

<blockquote>
<p>To disable auto-commit, simply delay your MarkOffset calls. A commit will only occur when the offsets have been changed. If you are not ready to commit, then don&rsquo;t mark the offset as ready.</p>
</blockquote>

<p>对于其中的另一个建议：即由我们主动调用<code>consumer.CommitOffsets()</code>。当然它最后补了一刀：</p>

<blockquote>
<p>Auto commit gives us some trouble in this case, as we might commit offsets which we have not yet written to the database. Having the ability to disable it by simply setting config.Consumer.Offsets.CommitInterval = 0 would be great.</p>
</blockquote>

<p>因为在<code>sarama/config</code>的校验配置中，做了如下的限制：当前库不支持手动提交<code>offset</code>:</p>

<pre><code>switch {
	case c.Consumer.Offsets.CommitInterval &lt;= 0:
		return ConfigurationError(&quot;Consumer.Offsets.CommitInterval must be &gt; 0&quot;)
</code></pre>

<h2 id="待解决的问题">待解决的问题</h2>

<p>常规来说，在调用第三方，完成支付时，采用的都是<code>http</code>异步通知的方式。即第三方不停的回调我们的接口，直到我们返回成功。或者，达到一个失败次数的阈值才停止。</p>

<p>但当前第三方重构，决定修改成发送<code>MQ</code>的方式。即支付成功之后，它只保证消息成功的被发送到了<code>kafka</code>。我们业务方订阅相应的<code>Topic</code>，来消费它推送的数据，去给用户添加权益。</p>

<p>当<code>Consumer</code>收到消息后，我们需要做一些本地的业务操作，比如写数据库。之后才能<code>MarkOffset</code>这条数据。但我们的操作绝对不可能100%成功，如果在<code>MarkOffset</code>前，某一步出错了，我们就直接返回。我们认为<strong>kafka还会给我们重新推送这条处理失败的消息</strong>。</p>

<p>测试的结果让我头疼：</p>

<ol>
<li>无法确定它下次尝试推送的时间</li>
<li>无法确定它是不是100%都会推送</li>
</ol>

<h2 id="方案可行性">方案可行性</h2>

<p>查阅了部分博客，感觉方案应该可行：<strong>只要不提交消息的<code>offset</code>，消息还会被重复推送</strong>。至少很多人也这么搞过，呵呵！比如下适用<code>Kafka</code>实现的一个分布式<code>DelayQueue</code>的介绍：</p>

<blockquote>
<p>Each produced message should have a timestamp at which it was pushed to the queue. At the
consumer side, fetch a message from a partition and compare the message timestamp with system&rsquo;s timestamp to see if enough time has passed for you
to process the message. If enough time has passed, process the message and commit the message&rsquo;s offset otherwise make sure you do not commit the
offset.</p>
</blockquote>

<p>因为我们之前也这样搞过！但这种方案不好，效率比较低，不如考虑<code>redis的Zset</code>来实现。当然，<code>zset</code>这也是我们后来采取的方案。</p>

<h2 id="了解-kafka">了解<code>kafka</code></h2>

<h3 id="consumer-使用-pull-拉取数据"><code>consumer</code>使用<code>pull</code>拉取数据</h3>

<blockquote>
<p>Kafka follows a more traditional design, shared by most messaging systems, where data is pushed to the broker from the producer and pulled from the broker by the consumer.</p>
</blockquote>

<h3 id="consumer-position"><code>consumer position</code></h3>

<blockquote>
<p>What is perhaps not obvious is that getting the broker and consumer to come into agreement about what has been consumed is not a trivial problem. If the broker records a message as consumed immediately every time it is handed out over the network, then if the consumer fails to process the message (say because it crashes or the request times out or whatever) that message will be lost. To solve this problem, many messaging systems add an acknowledgement feature which means that messages are only marked as sent not consumed when they are sent; the broker waits for a specific acknowledgement from the consumer to record the message as consumed. This strategy fixes the problem of losing messages, but creates new problems. First of all, if the consumer processes the message but fails before it can send an acknowledgement then the message will be consumed twice. The second problem is around performance, now the broker must keep multiple states about every single message (first to lock it so it is not given out a second time, and then to mark it as permanently consumed so that it can be removed). Tricky problems must be dealt with, like what to do with messages that are sent but never acknowledged.</p>

<p>Kafka handles this differently. Our topic is divided into a set of totally ordered partitions, each of which is consumed by exactly one consumer within each subscribing consumer group at any given time. This means that the position of a consumer in each partition is just a single integer, the offset of the next message to consume. This makes the state about what has been consumed very small, just one number for each partition. This state can be periodically checkpointed. This makes the equivalent of message acknowledgements very cheap.</p>
</blockquote>

<h3 id="offset-earliest-and-latest-的区别"><code>offset earliest and latest</code>的区别</h3>

<blockquote>
<p>It can read the messages, then save its position in the log, and finally process the messages. In this case there is a possibility that the consumer process crashes after saving its position but before saving the output of its message processing. In this case the process that took over processing would start at the saved position even though a few messages prior to that position had not been processed. This corresponds to &ldquo;at-most-once&rdquo; semantics as in the case of a consumer failure messages may not be processed.</p>

<p>It can read the messages, process the messages, and finally save its position. In this case there is a possibility that the consumer process crashes after processing messages but before saving its position. In this case when the new process takes over the first few messages it receives will already have been processed. This corresponds to the &ldquo;at-least-once&rdquo; semantics in the case of consumer failure. In many cases messages have a primary key and so the updates are idempotent (receiving the same message twice just overwrites a record with another copy of itself).</p>
</blockquote>

<h3 id="offsets-retention-配置属性"><code>Offsets.Retention</code>配置属性</h3>

<blockquote>
<p>The Kafka cluster durably persists all published records—whether or not they have been consumed—using a configurable retention period. For example, if the retention policy is set to two days, then for the two days after a record is published, it is available for consumption, after which it will be discarded to free up space. Kafka&rsquo;s performance is effectively constant with respect to data size so storing data for a long time is not a problem.</p>
</blockquote>

    </article>
</div>


        
<nav role="navigation" class="bottom-menu">
    

    
        <a href="/blog">back</a>
        
    

    
</p>
</nav>

    </main>
    
    <footer class="container footer"></footer>
    
    
</body>

</html>